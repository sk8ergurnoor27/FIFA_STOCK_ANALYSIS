# -*- coding: utf-8 -*-
"""FIFA _STOCK _ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mruP5BTOU0hJ3vGrXtojWLtLZPd7VGi4

# IMPORTING LIBRARIES
"""

! pip install pandas numpy yfinance requests beautifulsoup4 matplotlib seaborn plotly nltk transformers torch scikit-learn python-dateutil

pip install --upgrade plotly

# 2. Import all necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import yfinance as yf

def get_sample_data():
    # Get EA stock data
    ea_stock = yf.download('EA', start='2010-01-01', end='2023-12-31')
    ea_stock = ea_stock.reset_index()
    ea_stock['Year'] = ea_stock['Date'].dt.year

    # Create simulated game sales data
    years = range(2010, 2024)
    sales = [2.5, 3.1, 4.2, 5.3, 6.1, 7.4, 8.2, 10.1, 12.4, 14.2, 15.6, 16.8, 17.2, 18.1]
    fifa_sales = pd.DataFrame({'Year': years, 'Sales (millions)': sales})

    # Calculate annual stock averages
    annual_stock = ea_stock.groupby('Year')['Close'].mean().reset_index()

    # Ensure both DataFrames are simple (no multi-index)
    annual_stock = annual_stock.copy()
    fifa_sales = fifa_sales.copy()

    # Merge data - explicitly specify columns
    merged_data = pd.merge(
        annual_stock,
        fifa_sales,
        on='Year',
        how='inner'
    )

    return merged_data

"""# INITIALIZE GEN AI COMPONENTS"""

# Import necessary libraries
import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Initialize Gen AI components
nltk.download('vader_lexicon')
sentiment_analyzer = SentimentIntensityAnalyzer()
finbert = pipeline('text-classification', model='yiyanghkust/finbert-tone')

# Configuration
plt.style.use('ggplot')
pd.set_option('display.max_columns', None)

"""# FETCHING DATA FROM REDDIT"""

def fetch_stock_data(ticker, start_date, end_date):
    """
    Fetch historical stock data from Yahoo Finance
    """
    data = yf.download(ticker, start=start_date, end=end_date)
    return data

def fetch_game_sales_data():
    """
    Simulate fetching FIFA game sales data (in a real project, this would come from EA reports or market research)
    """
    years = range(2010, 2024)
    sales = [2.5, 3.1, 4.2, 5.3, 6.1, 7.4, 8.2, 10.1, 12.4, 14.2, 15.6, 16.8, 17.2, 18.1]
    return pd.DataFrame({'Year': years, 'Sales (millions)': sales})

def fetch_reddit_sentiment(query, limit=100):
    """
    Fetch Reddit posts for sentiment analysis (simplified for this example)
    """
    # In a real project, you would use the Reddit API
    # Here we'll simulate some sample posts
    posts = [
        "FIFA 23 is the best game ever! The graphics are amazing.",
        "EA is just recycling the same game every year with minor updates.",
        "The Ultimate Team mode is pay-to-win and ruins the experience.",
        "I love the new career mode features in FIFA 24!",
        "EA's stock might drop after the latest controversy about loot boxes."
    ]
    return pd.DataFrame({'text': posts})

# Fetch data
ea_stock = fetch_stock_data('EA', '2010-01-01', '2023-12-31')
fifa_sales = fetch_game_sales_data()
reddit_posts = fetch_reddit_sentiment('FIFA')

import yfinance as yf
ea_stock = yf.download("EA", start="2010-01-01", end="2023-12-31")

print("Columns in downloaded data:", ea_stock.columns.tolist())

"""# COLUMNS MODIFICATION ACCORDING TO OUR NEEDS"""

# Flatten columns (keep only the first level: 'Close', 'High', etc.)
ea_stock.columns = ea_stock.columns.droplevel(1)
print(ea_stock.columns)  # Now shows: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')

ea_stock = yf.download("EA", start="2010-01-01", end="2023-12-31", auto_adjust=True)
print(ea_stock.columns)  # Clean single-level columns

"""# SUMMARY OF THE DATA COLLECTED"""

import yfinance as yf
import pandas as pd

# Download data
ea_stock = yf.download("EA", start="2010-01-01", end="2023-12-31", auto_adjust=True)

# Debug: Print columns and sample data
print("Columns:", ea_stock.columns.tolist())
print("\nSample data:")
print(ea_stock.head())

# Flatten multi-level columns if they exist
if isinstance(ea_stock.columns, pd.MultiIndex):
    ea_stock.columns = ea_stock.columns.droplevel(1)

# Handle column name variations
close_col = 'Close' if 'Close' in ea_stock.columns else 'Adj Close'
volume_col = 'Volume' if 'Volume' in ea_stock.columns else 'Vol'

# Reset index and add year/month
ea_stock = ea_stock.reset_index()
ea_stock['Year'] = ea_stock['Date'].dt.year
ea_stock['Month'] = ea_stock['Date'].dt.month

# Group by year
annual_stock = ea_stock.groupby('Year').agg({
    close_col: 'mean',
    volume_col: 'sum'
}).reset_index()

print("\nAnnual aggregated data:")
print(annual_stock.head())

"""# SIMULATED VERSION OF LLM MODEL"""

# 1. Import Libraries
import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# For OpenAI API (if you use it)
# import openai
# from dotenv import load_dotenv
# import os
# load_dotenv() # Load environment variables from .env file
# openai.api_key = os.getenv("OPENAI_API_KEY") # Or directly set it if not using .env

# Initialize Gen AI components
nltk.download('vader_lexicon')
sentiment_analyzer = SentimentIntensityAnalyzer()
finbert = pipeline('text-classification', model='yiyanghkust/finbert-tone', tokenizer='yiyanghkust/finbert-tone')

# Configuration for plots
plt.style.use('ggplot')
pd.set_option('display.max_columns', None)

# 2. Data Fetching Functions
def fetch_stock_data(ticker, start_date, end_date):
    """
    Fetches historical stock data from Yahoo Finance for a given ticker and date range.
    Ensures single-level columns for easier access.
    """
    try:
        data = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.droplevel(1)
        data = data.reset_index() # Ensure Date is a column
        return data
    except Exception as e:
        print(f"Error fetching stock data for {ticker}: {e}")
        return pd.DataFrame()

def fetch_game_sales_data():
    """
    Simulates fetching FIFA game sales data.
    In a real project, this would come from EA reports or market research APIs.
    """
    years = range(2010, 2024)
    sales = [2.5, 3.1, 4.2, 5.3, 6.1, 7.4, 8.2, 10.1, 12.4, 14.2, 15.6, 16.8, 17.2, 18.1]
    return pd.DataFrame({'Year': years, 'Sales (millions)': sales})

def fetch_reddit_sentiment(query, limit=100):
    """
    Simulates fetching Reddit posts for sentiment analysis.
    In a real project, you would use the Reddit API (e.g., PRAW).
    """
    posts = [
        "FIFA 23 is the best game ever! The graphics are amazing.",
        "EA is just recycling the same game every year with minor updates.",
        "The Ultimate Team mode is pay-to-win and ruins the experience.",
        "I love the new career mode features in FIFA 24!",
        "EA's stock might drop after the latest controversy about loot boxes."
    ]
    return pd.DataFrame({'text': posts})

# 3. Sentiment Analysis Function
def analyze_sentiment(text):
    """
    Analyzes sentiment of a given text using VADER and FinBERT.
    Returns a combined sentiment score.
    """
    sia_score = sentiment_analyzer.polarity_scores(text)['compound']
    try:
        finbert_result = finbert(text)[0]
        # FinBERT labels are typically 'positive', 'negative', 'neutral'
        finbert_score = 1 if finbert_result['label'] == 'positive' else -1 if finbert_result['label'] == 'negative' else 0
        finbert_score *= finbert_result['score']
    except Exception as e:
        # print(f"FinBERT error for text '{text[:50]}...': {e}") # Uncomment for debugging
        finbert_score = 0
    return (sia_score + finbert_score) / 2

# 4. Data Loading and Initial Processing
ea_stock = fetch_stock_data('EA', '2010-01-01', '2023-12-31')
fifa_sales = fetch_game_sales_data()
reddit_posts = fetch_reddit_sentiment('FIFA')

# Process stock data for merging
ea_stock['Year'] = ea_stock['Date'].dt.year
ea_stock['Month'] = ea_stock['Date'].dt.month

# Calculate annual averages for comparison with game sales
annual_stock = ea_stock.groupby('Year').agg({
    'Close': 'mean',
    'Volume': 'sum'
}).reset_index()

# Merge stock and sales data
merged_data = pd.merge(annual_stock, fifa_sales, on='Year', how='left')

# Calculate correlation
correlation = merged_data[['Close', 'Sales (millions)']].corr().iloc[0,1]

# Apply sentiment analysis to Reddit posts
reddit_posts['sentiment'] = reddit_posts['text'].apply(analyze_sentiment)
avg_sentiment = reddit_posts['sentiment'].mean()

"""# ML MODEL"""

# 6. Model Training and Evaluation
# Define features (X) and target (y)
features = ['sales_lag1', 'price_lag1', 'sales_ma3', 'price_ma3', 'sentiment']
# Ensure all features exist in model_data before selecting
if not all(f in model_data.columns for f in features):
    print(f"Missing features in model_data: {set(features) - set(model_data.columns)}")
    # Handle this error, e.g., by exiting or using a subset of features
    raise ValueError("Required features for model training are missing.")

X = model_data[features]
y = model_data['Close']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Evaluate model performance
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Get feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False).reset_index(drop=True)

print("\n--- Model Training and Evaluation Complete ---")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R²): {r2:.4f}")
print("Feature Importance:\n", feature_importance)

"""# INTERACTIVE DASHBOARD USING PLOTLY"""

pip install plotly --upgrade

import plotly.io as pio
pio.renderers.default = 'notebook'  # or 'browser' if you want it to open in a new window

pip install --upgrade plotly

# 0. Initial Setup and Library Installation (Run once)
# pip install pandas numpy yfinance requests beautifulsoup4 matplotlib seaborn plotly nltk transformers torch scikit-learn python-dateutil --quiet
# pip install --upgrade plotly --quiet
# pip install openai python-dotenv --quiet # If you plan to use OpenAI API



print("--- Data Loading and Initial Processing Complete ---")
print("Merged Data Head:\n", merged_data.head())
print(f"Correlation (Stock vs Sales): {correlation:.2f}")
print(f"Average Reddit Sentiment: {avg_sentiment:.2f}")# 1. Import Libraries
import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# For OpenAI API (if you use it)
# import openai
# from dotenv import load_dotenv
# import os
# load_dotenv() # Load environment variables from .env file
# openai.api_key = os.getenv("OPENAI_API_KEY") # Or directly set it if not using .env

# Initialize Gen AI components
nltk.download('vader_lexicon')
sentiment_analyzer = SentimentIntensityAnalyzer()
finbert = pipeline('text-classification', model='yiyanghkust/finbert-tone', tokenizer='yiyanghkust/finbert-tone')

# Configuration for plots
plt.style.use('ggplot')
pd.set_option('display.max_columns', None)

# 2. Data Fetching Functions
def fetch_stock_data(ticker, start_date, end_date):
    """
    Fetches historical stock data from Yahoo Finance for a given ticker and date range.
    Ensures single-level columns for easier access.
    """
    try:
        data = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.droplevel(1)
        data = data.reset_index() # Ensure Date is a column
        return data
    except Exception as e:
        print(f"Error fetching stock data for {ticker}: {e}")
        return pd.DataFrame()

def fetch_game_sales_data():
    """
    Simulates fetching FIFA game sales data.
    In a real project, this would come from EA reports or market research APIs.
    """
    years = range(2010, 2024)
    sales = [2.5, 3.1, 4.2, 5.3, 6.1, 7.4, 8.2, 10.1, 12.4, 14.2, 15.6, 16.8, 17.2, 18.1]
    return pd.DataFrame({'Year': years, 'Sales (millions)': sales})

def fetch_reddit_sentiment(query, limit=100):
    """
    Simulates fetching Reddit posts for sentiment analysis.
    In a real project, you would use the Reddit API (e.g., PRAW).
    """
    posts = [
        "FIFA 23 is the best game ever! The graphics are amazing.",
        "EA is just recycling the same game every year with minor updates.",
        "The Ultimate Team mode is pay-to-win and ruins the experience.",
        "I love the new career mode features in FIFA 24!",
        "EA's stock might drop after the latest controversy about loot boxes."
    ]
    return pd.DataFrame({'text': posts})

# 3. Sentiment Analysis Function
def analyze_sentiment(text):
    """
    Analyzes sentiment of a given text using VADER and FinBERT.
    Returns a combined sentiment score.
    """
    sia_score = sentiment_analyzer.polarity_scores(text)['compound']
    try:
        finbert_result = finbert(text)[0]
        # FinBERT labels are typically 'positive', 'negative', 'neutral'
        finbert_score = 1 if finbert_result['label'] == 'positive' else -1 if finbert_result['label'] == 'negative' else 0
        finbert_score *= finbert_result['score']
    except Exception as e:
        # print(f"FinBERT error for text '{text[:50]}...': {e}") # Uncomment for debugging
        finbert_score = 0
    return (sia_score + finbert_score) / 2

# 4. Data Loading and Initial Processing
ea_stock = fetch_stock_data('EA', '2010-01-01', '2023-12-31')
fifa_sales = fetch_game_sales_data()
reddit_posts = fetch_reddit_sentiment('FIFA')

# Process stock data for merging
ea_stock['Year'] = ea_stock['Date'].dt.year
ea_stock['Month'] = ea_stock['Date'].dt.month

# Calculate annual averages for comparison with game sales
annual_stock = ea_stock.groupby('Year').agg({
    'Close': 'mean',
    'Volume': 'sum'
}).reset_index()

# Merge stock and sales data
merged_data = pd.merge(annual_stock, fifa_sales, on='Year', how='left')

# Calculate correlation
correlation = merged_data[['Close', 'Sales (millions)']].corr().iloc[0,1]

# Apply sentiment analysis to Reddit posts
reddit_posts['sentiment'] = reddit_posts['text'].apply(analyze_sentiment)
avg_sentiment = reddit_posts['sentiment'].mean()

# 5. Feature Engineering for Model
def prepare_model_data(data_to_process):
    """
    Prepares data for the predictive model by adding lag features,
    moving averages, and a simulated sentiment feature.
    """
    model_data = data_to_process.copy()

    # Lag features
    model_data['sales_lag1'] = model_data['Sales (millions)'].shift(1)
    model_data['price_lag1'] = model_data['Close'].shift(1)

    # Moving averages (e.g., 3-year rolling mean)
    model_data['sales_ma3'] = model_data['Sales (millions)'].rolling(3).mean()
    model_data['price_ma3'] = model_data['Close'].rolling(3).mean()

    # Add sentiment (using the calculated average sentiment for simplicity,
    # ideally this would be time-series aligned sentiment)
    model_data['sentiment'] = avg_sentiment # Using the global average for now

    # Drop rows with NaN values created by lag/rolling features
    model_data = model_data.dropna().reset_index(drop=True)

    return model_data

model_data = prepare_model_data(merged_data)

print("\n--- Feature Engineering Complete ---")
print("Model Data Head:\n", model_data.head())

# 6. Model Training and Evaluation
# Define features (X) and target (y)
features = ['sales_lag1', 'price_lag1', 'sales_ma3', 'price_ma3', 'sentiment']
# Ensure all features exist in model_data before selecting
if not all(f in model_data.columns for f in features):
    print(f"Missing features in model_data: {set(features) - set(model_data.columns)}")
    # Handle this error, e.g., by exiting or using a subset of features
    raise ValueError("Required features for model training are missing.")

X = model_data[features]
y = model_data['Close']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Evaluate model performance
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Get feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False).reset_index(drop=True)

print("\n--- Model Training and Evaluation Complete ---")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R²): {r2:.4f}")
print("Feature Importance:\n", feature_importance)

# 7. AI-Generated Insights
def generate_ai_insights(data, correlation, avg_sentiment_score, mse_score, r2_score):
    """
    Generates simulated AI-powered investment insights based on the analysis.
    In a real application, this would integrate with an actual LLM API.
    """
    # Calculate key metrics for simulated insights
    if not data.empty:
        stock_growth = (data['Close'].iloc[-1] - data['Close'].iloc[0]) / data['Close'].iloc[0] * 100
        best_year = data.loc[data['Close'].idxmax()]['Year']
        worst_year = data.loc[data['Close'].idxmin()]['Year']
    else:
        stock_growth = 0
        best_year = "N/A"
        worst_year = "N/A"

    insights = f"""
    ## AI-Generated Investment Insights

    **Key Findings:**
    - EA's stock has shown an overall {'growth' if stock_growth > 0 else 'decline'} of {abs(stock_growth):.1f}% since 2010.
    - The strongest year for EA stock was {best_year}, while the most challenging was {worst_year}.
    - FIFA game sales and EA stock price show a {'strong' if correlation > 0.7 else 'moderate' if correlation > 0.4 else 'weak'} correlation ({correlation:.2f}).
    - Recent sentiment analysis suggests {'positive' if avg_sentiment_score > 0.2 else 'negative' if avg_sentiment_score < -0.2 else 'neutral'} consumer sentiment (Average score: {avg_sentiment_score:.2f}).
    - The predictive model achieved an MSE of {mse_score:.4f} and an R-squared of {r2_score:.4f}, indicating {'good' if r2_score > 0.7 else 'moderate' if r2_score > 0.4 else 'limited'} predictive power.

    **Recommendations:**
    - {'Consider increasing position' if avg_sentiment_score > 0.5 and correlation > 0.6 else 'Hold position' if avg_sentiment_score > -0.3 else 'Consider reducing exposure'}.
    - Monitor upcoming game release cycles and regulatory changes in microtransaction policies.
    - Track competitor performance in the sports gaming segment.
    - Further research into specific game releases and their immediate stock impact could provide deeper insights.
    """
    return insights

ai_insights = generate_ai_insights(merged_data, correlation, avg_sentiment, mse, r2)
print("\n" + ai_insights)


# 8. Plotly Dashboard Creation
# Set Plotly renderer (important for Colab/Jupyter)
import plotly.io as pio
pio.renderers.default = 'notebook' # or 'browser' if you want it to open in a new window

def create_dashboard():
    """
    Creates a comprehensive Plotly dashboard visualizing stock data,
    game sales, sentiment, model performance, and feature importance.
    """
    try:
        fig = make_subplots(
            rows=3, cols=2,
            specs=[
                [{"type": "scatter", "colspan": 2}, None],
                [{"type": "xy", "secondary_y": True}, {"type": "histogram"}],
                [{"type": "bar"}, {"type": "xy"}]
            ],
            subplot_titles=(
                "EA Stock Price History (2010-2023)",
                "Stock Price vs FIFA Game Sales",
                "Reddit Sentiment Analysis",
                "Random Forest Feature Importance",
                "Actual vs Predicted Stock Prices"
            ),
            vertical_spacing=0.15,
            horizontal_spacing=0.1,
            column_widths=[0.6, 0.4]
        )

        # 1. Stock Price History (Top)
        fig.add_trace(
            go.Scatter(
                x=ea_stock['Date'],
                y=ea_stock['Close'],
                name="EA Stock Price",
                line=dict(color='#00FFFF', width=2),
                hovertemplate="Date: %{x}<br>Price: $%{y:.2f}<extra></extra>"
            ),
            row=1, col=1
        )

        # 2. Stock Price vs Game Sales (Middle Left)
        fig.add_trace(
            go.Scatter(
                x=merged_data['Year'],
                y=merged_data['Close'],
                name="Stock Price",
                line=dict(color='#1f77b4', width=3),
                hovertemplate="Year: %{x}<br>Price: $%{y:.2f}<extra></extra>"
            ),
            row=2, col=1
        )
        fig.add_trace(
            go.Scatter(
                x=merged_data['Year'],
                y=merged_data['Sales (millions)'],
                name="FIFA Sales (Millions)",
                line=dict(color='#ff7f0e', width=3, dash='dot'),
                hovertemplate="Year: %{x}<br>Sales: %{y}M<extra></extra>"
            ),
            row=2, col=1,
            secondary_y=True
        )

        # 3. Sentiment Analysis (Middle Right)
        fig.add_trace(
            go.Histogram(
                x=reddit_posts['sentiment'],
                name="Sentiment",
                marker_color='#2ca02c',
                opacity=0.7,
                nbinsx=20,
                hovertemplate="Sentiment: %{x}<br>Count: %{y}<extra></extra>"
            ),
            row=2, col=2
        )

        # 4. Feature Importance (Bottom Left)
        fig.add_trace(
            go.Bar(
                x=feature_importance['Importance'],
                y=feature_importance['Feature'],
                name="Feature Importance",
                marker_color='#d62728',
                orientation='h',
                hovertemplate="Importance: %{x}<br>Feature: %{y}<extra></extra>"
            ),
            row=3, col=1
        )

        # 5. Actual vs Predicted (Bottom Right)
        fig.add_trace(
            go.Scatter(
                x=y_test, # Actual values on X-axis
                y=predictions, # Predicted values on Y-axis
                name="Predictions",
                mode='markers',
                marker=dict(color='#9467bd', size=10),
                hovertemplate="Actual: $%{x:.2f}<br>Predicted: $%{y:.2f}<extra></extra>"
            ),
            row=3, col=2
        )
        fig.add_trace(
            go.Scatter(
                x=[y_test.min(), y_test.max()],
                y=[y_test.min(), y_test.max()],
                name="Perfect Prediction",
                line=dict(color='white', dash='dash'),
                hovertemplate=None # No hover for this line
            ),
            row=3, col=2
        )

        # Update layout
        fig.update_layout(
            title_text="<b>FIFA Stock Analysis Dashboard</b>",
            title_font_size=24,
            title_x=0.5,
            height=1400,
            width=1200,
            showlegend=True,
            template='plotly_dark',
            hovermode='closest',

        )

        # Update axes
        fig.update_yaxes(title_text="Stock Price ($)", row=1, col=1)
        fig.update_yaxes(title_text="Stock Price ($)", row=2, col=1)
        fig.update_yaxes(title_text="Game Sales (Millions)", secondary_y=True, row=2, col=1)
        fig.update_xaxes(title_text="Sentiment Score", row=2, col=2)
        fig.update_yaxes(title_text="Count", row=2, col=2)
        fig.update_xaxes(title_text="Feature Importance", row=3, col=1)
        fig.update_xaxes(title_text="Actual Price ($)", row=3, col=2)
        fig.update_yaxes(title_text="Predicted Price ($)", row=3, col=2)

        return fig

    except Exception as e:
        print(f"Error creating dashboard: {str(e)}")
        return None

# Create and show dashboard
dashboard = create_dashboard()
if dashboard:
    dashboard.show()
else:
    print("Failed to create dashboard. Please check your data and model outputs.")

"""# EXPORTING EXCEL FILE FOR POWERBI AND TABLEAU DASHBOARDS"""

# Ensure you have openpyxl installed for pandas to write to .xlsx format
# !pip install openpyxl --quiet

# --- Add this code block to the end of your existing Python script ---

print("\n--- Exporting data to Excel for Power BI ---")

excel_file_name = "FIFA_Stock_Analysis_Data.xlsx"

try:
    with pd.ExcelWriter(excel_file_name, engine='openpyxl') as writer:
        # 1. Stock_Data_Daily
        # Ensure 'Date' column is in datetime format before exporting
        ea_stock['Date'] = pd.to_datetime(ea_stock['Date'])
        ea_stock.to_excel(writer, sheet_name='Stock_Data_Daily', index=False)
        print(f"Exported 'Stock_Data_Daily' sheet with {len(ea_stock)} rows.")

        # 2. Annual_Aggregated_Data
        merged_data.to_excel(writer, sheet_name='Annual_Aggregated_Data', index=False)
        print(f"Exported 'Annual_Aggregated_Data' sheet with {len(merged_data)} rows.")

        # 3. Sentiment_Data
        reddit_posts.to_excel(writer, sheet_name='Sentiment_Data', index=False)
        print(f"Exported 'Sentiment_Data' sheet with {len(reddit_posts)} rows.")

        # 4. Model_Performance (Actual vs Predicted)
        # Create a DataFrame for actual vs predicted values
        # y_test is a Series, predictions is a numpy array. Align them by index.
        model_performance_df = pd.DataFrame({
            'Actual_Price': y_test.values,
            'Predicted_Price': predictions
        }, index=y_test.index) # Keep original index for potential time alignment if needed
        model_performance_df.reset_index(inplace=True) # Convert index to column if it's meaningful (e.g., Year)
        # If y_test.index is just a range, you might want to add a 'Data_Point_ID'
        if 'Year' in model_data.columns and 'index' in model_performance_df.columns:
             # Map the original Year from model_data to the split data
            model_performance_df['Year'] = model_data.loc[model_performance_df['index'], 'Year'].values
            model_performance_df.drop(columns=['index'], inplace=True)
        else:
            model_performance_df.rename(columns={'index': 'Data_Point_ID'}, inplace=True)

        model_performance_df.to_excel(writer, sheet_name='Model_Performance', index=False)
        print(f"Exported 'Model_Performance' sheet with {len(model_performance_df)} rows.")

        # 5. Feature_Importance
        feature_importance.to_excel(writer, sheet_name='Feature_Importance', index=False)
        print(f"Exported 'Feature_Importance' sheet with {len(feature_importance)} rows.")

        # 6. Key_Metrics
        # Recalculate stock_growth, best_year, worst_year to ensure they are available
        if not merged_data.empty:
            stock_growth = (merged_data['Close'].iloc[-1] - merged_data['Close'].iloc[0]) / merged_data['Close'].iloc[0] * 100
            best_year = merged_data.loc[merged_data['Close'].idxmax()]['Year']
            worst_year = merged_data.loc[merged_data['Close'].idxmin()]['Year']
        else:
            stock_growth = np.nan
            best_year = "N/A"
            worst_year = "N/A"

        key_metrics_df = pd.DataFrame({
            'Metric': [
                'Overall Stock Growth (%)',
                'Correlation (Stock vs Sales)',
                'Average Reddit Sentiment',
                'Model MSE',
                'Model R-squared',
                'Strongest Stock Year',
                'Weakest Stock Year'
            ],
            'Value': [
                f"{stock_growth:.2f}",
                f"{correlation:.2f}",
                f"{avg_sentiment:.2f}",
                f"{mse:.4f}",
                f"{r2:.4f}",
                str(best_year),
                str(worst_year)
            ]
        })
        key_metrics_df.to_excel(writer, sheet_name='Key_Metrics', index=False)
        print(f"Exported 'Key_Metrics' sheet.")

    print(f"\nSuccessfully created '{excel_file_name}' for Power BI.")

except Exception as e:
    print(f"Error exporting data to Excel: {e}")

